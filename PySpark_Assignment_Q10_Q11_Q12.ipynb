{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSL7110 Assignment 1 (Questions 11 and 12)\n",
        "\n",
        "**Submitted By: Arjun Baidya (M25CSA006)**"
      ],
      "metadata": {
        "id": "g992BGvGacPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyPJbQ50yoCJ",
        "outputId": "04df89e8-91f7-45e6-f0d7-7c94835378ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q D184MB.zip -d dataset"
      ],
      "metadata": {
        "id": "wzKPOb_myuvb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import re\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import os\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSL7110_Assignment\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark Session created successfully!\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO_YDLXI3w8N",
        "outputId": "b4f98c0f-4dc6-4020-83f4-7bd951b199db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n",
            "Spark Session created successfully!\n",
            "Spark Version: 4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/dataset/D184MB/'\n",
        "\n",
        "def load_books(data_path):\n",
        "    #Loading all .txt files into a DataFrame\n",
        "\n",
        "    books_data = []\n",
        "    txt_files = glob(os.path.join(data_path, '*.txt'))\n",
        "\n",
        "    print(f\"Found {len(txt_files)} text files\")\n",
        "\n",
        "    for file_path in txt_files[:100]:  # Limit to first 100 for faster testing\n",
        "        file_name = os.path.basename(file_path)\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                text = f.read()\n",
        "                books_data.append((file_name, text))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_name}: {e}\")\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"file_name\", StringType(), False),\n",
        "        StructField(\"text\", StringType(), False)\n",
        "    ])\n",
        "\n",
        "    return spark.createDataFrame(books_data, schema)\n",
        "\n",
        "# Loading data\n",
        "books_df = load_books(DATA_PATH)\n",
        "books_df.cache()\n",
        "\n",
        "print(f\"\\nLoaded {books_df.count()} books successfully!\")\n",
        "books_df.select(\"file_name\", substring(\"text\", 1, 50).alias(\"preview\")).show(5, truncate=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le1E-Bsf4Qlj",
        "outputId": "83069aab-f51a-4593-8fe6-d7f2b746f641"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 425 text files\n",
            "\n",
            "Loaded 100 books successfully!\n",
            "+---------+--------------------------------------------------+\n",
            "|file_name|                                           preview|\n",
            "+---------+--------------------------------------------------+\n",
            "|  423.txt|The Project Gutenberg EBook of Round the Red Lamp,|\n",
            "|  152.txt|The Project Gutenberg EBook of Wild Justice, by Ru|\n",
            "|   26.txt|The Project Gutenberg EBook of Paradise Lost, by J|\n",
            "|   81.txt|The Project Gutenberg EBook of The Return of Tarza|\n",
            "|  456.txt|Project Gutenberg's The Door in the Wall And Other|\n",
            "+---------+--------------------------------------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10\n",
        "\n",
        "def extract_title(text):\n",
        "    if not text: return None\n",
        "    match = re.search(r'Title:\\s*(.+?)(?:\\r?\\n)', text, re.IGNORECASE)\n",
        "    if match: return match.group(1).strip()\n",
        "    match = re.search(r'The Project Gutenberg EBook of\\s*(.+?)(?:,|\\r?\\n)', text, re.IGNORECASE)\n",
        "    if match: return match.group(1).strip()\n",
        "    return None\n",
        "\n",
        "def extract_release_date(text):\n",
        "    if not text: return None\n",
        "    match = re.search(r'Release Date:\\s*(.+?)(?:\\[|$|\\r?\\n)', text, re.IGNORECASE)\n",
        "    if match: return match.group(1).strip()\n",
        "    return None\n",
        "\n",
        "def extract_language(text):\n",
        "    if not text: return None\n",
        "    match = re.search(r'Language:\\s*(.+?)(?:\\r?\\n)', text, re.IGNORECASE)\n",
        "    if match: return match.group(1).strip()\n",
        "    return None\n",
        "\n",
        "def extract_encoding(text):\n",
        "    if not text: return None\n",
        "    match = re.search(r'Character set encoding:\\s*(.+?)(?:\\r?\\n)', text, re.IGNORECASE)\n",
        "    if match: return match.group(1).strip()\n",
        "    return None\n",
        "\n",
        "def parse_year(date_str):\n",
        "    if not date_str: return None\n",
        "    match = re.search(r'\\b(19|20)\\d{2}\\b', str(date_str))\n",
        "    if match: return int(match.group(0))\n",
        "    return None\n",
        "\n",
        "# Registering UDFs\n",
        "extract_title_udf = udf(extract_title, StringType())\n",
        "extract_release_date_udf = udf(extract_release_date, StringType())\n",
        "extract_language_udf = udf(extract_language, StringType())\n",
        "extract_encoding_udf = udf(extract_encoding, StringType())\n",
        "parse_year_udf = udf(parse_year, IntegerType())\n",
        "\n",
        "# Extracting Metadata\n",
        "metadata_df = books_df.select(\n",
        "    col(\"file_name\"),\n",
        "    extract_title_udf(col(\"text\")).alias(\"title\"),\n",
        "    extract_release_date_udf(col(\"text\")).alias(\"release_date\"),\n",
        "    extract_language_udf(col(\"text\")).alias(\"language\"),\n",
        "    extract_encoding_udf(col(\"text\")).alias(\"encoding\")\n",
        ").withColumn(\n",
        "    \"release_year\",\n",
        "    parse_year_udf(col(\"release_date\"))\n",
        ")\n",
        "\n",
        "print(\"QUESTION 10 - METADATA EXTRACTION\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n--- Sample Metadata ---\")\n",
        "metadata_df.show(10, truncate=50)\n",
        "\n",
        "print(\"\\nAnalysis 1: Books Released Each Year\")\n",
        "books_per_year = metadata_df.filter(col(\"release_year\").isNotNull()) \\\n",
        "    .groupBy(\"release_year\") \\\n",
        "    .count() \\\n",
        "    .orderBy(\"release_year\")\n",
        "\n",
        "books_per_year.show(20)\n",
        "\n",
        "print(\"\\nAnalysis 2: Most Common Language\")\n",
        "language_counts = metadata_df.filter(col(\"language\").isNotNull()) \\\n",
        "    .groupBy(\"language\") \\\n",
        "    .count() \\\n",
        "    .orderBy(desc(\"count\"))\n",
        "\n",
        "language_counts.show(10)\n",
        "\n",
        "most_common = language_counts.first()\n",
        "if most_common:\n",
        "    print(f\"\\nMost common language: {most_common['language']} ({most_common['count']} books)\")\n",
        "\n",
        "print(\"\\nAnalysis 3: Average Title Length\")\n",
        "title_length_df = metadata_df.filter(col(\"title\").isNotNull()) \\\n",
        "    .withColumn(\"title_length\", length(col(\"title\")))\n",
        "\n",
        "avg_length = title_length_df.agg(avg(\"title_length\")).first()[0]\n",
        "print(f\"Average title length: {avg_length:.2f} characters\")\n",
        "\n",
        "title_length_df.describe(\"title_length\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlQbQ0n84kjz",
        "outputId": "fa0f06cb-0747-4032-d78e-2bf423d779f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION 10 - METADATA EXTRACTION\n",
            "================================================================================\n",
            "\n",
            "--- Sample Metadata ---\n",
            "+---------+---------------------------------------+----------------+--------+--------+------------+\n",
            "|file_name|                                  title|    release_date|language|encoding|release_year|\n",
            "+---------+---------------------------------------+----------------+--------+--------+------------+\n",
            "|  423.txt|                     Round the Red Lamp|February 3, 2008| English|   ASCII|        2008|\n",
            "|  152.txt|                           Wild Justice|    August, 1994| English|   ASCII|        1994|\n",
            "|   26.txt|                          Paradise Lost|  February, 1992| English|   ASCII|        1992|\n",
            "|   81.txt|                   The Return of Tarzan|   June 23, 2008| English|   ASCII|        2008|\n",
            "|  456.txt| The Door in the Wall And Other Stories|   July 22, 2005| English|   ASCII|        2005|\n",
            "|  148.txt| The Autobiography of Benjamin Franklin|    May 22, 2008| English|   ASCII|        2008|\n",
            "|  323.txt|                       Verses 1889-1896|   June 29, 2008| English|   ASCII|        2008|\n",
            "|  465.txt|                          The Mountains|     March, 1996| English|   ASCII|        1996|\n",
            "|   30.txt|The Bible, King James Version, Complete|     April, 1992| English|   ASCII|        1992|\n",
            "|  286.txt|                                 Laddie|   April 3, 2008| English|   ASCII|        2008|\n",
            "+---------+---------------------------------------+----------------+--------+--------+------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Analysis 1: Books Released Each Year\n",
            "+------------+-----+\n",
            "|release_year|count|\n",
            "+------------+-----+\n",
            "|        1979|    1|\n",
            "|        1992|    8|\n",
            "|        1993|    1|\n",
            "|        1994|    5|\n",
            "|        1995|   11|\n",
            "|        1996|    9|\n",
            "|        2005|    1|\n",
            "|        2006|   11|\n",
            "|        2007|    3|\n",
            "|        2008|   39|\n",
            "|        2010|    5|\n",
            "|        2011|    1|\n",
            "|        2012|    1|\n",
            "|        2013|    1|\n",
            "+------------+-----+\n",
            "\n",
            "\n",
            "Analysis 2: Most Common Language\n",
            "+--------+-----+\n",
            "|language|count|\n",
            "+--------+-----+\n",
            "| English|   95|\n",
            "|   Latin|    2|\n",
            "+--------+-----+\n",
            "\n",
            "\n",
            "Most common language: English (95 books)\n",
            "\n",
            "Analysis 3: Average Title Length\n",
            "Average title length: 23.81 characters\n",
            "+-------+------------------+\n",
            "|summary|      title_length|\n",
            "+-------+------------------+\n",
            "|  count|                97|\n",
            "|   mean|23.814432989690722|\n",
            "| stddev| 12.50110819829854|\n",
            "|    min|                 6|\n",
            "|    max|                60|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 11: TF-IDF AND BOOK SIMILARITY\n",
        "\n",
        "def remove_gutenberg_boilerplate(text):\n",
        "    if not text: return \"\"\n",
        "\n",
        "    start_markers = [\n",
        "        \"*** START OF THIS PROJECT GUTENBERG\",\n",
        "        \"*** START OF THE PROJECT GUTENBERG\"\n",
        "    ]\n",
        "\n",
        "    start_pos = 0\n",
        "    for marker in start_markers:\n",
        "        pos = text.find(marker)\n",
        "        if pos != -1:\n",
        "            newline_pos = text.find('\\n', pos)\n",
        "            if newline_pos != -1:\n",
        "                start_pos = newline_pos + 1\n",
        "                break\n",
        "\n",
        "    end_markers = [\n",
        "        \"*** END OF THIS PROJECT GUTENBERG\",\n",
        "        \"*** END OF THE PROJECT GUTENBERG\"\n",
        "    ]\n",
        "\n",
        "    end_pos = len(text)\n",
        "    for marker in end_markers:\n",
        "        pos = text.find(marker)\n",
        "        if pos != -1:\n",
        "            end_pos = pos\n",
        "            break\n",
        "\n",
        "    return text[start_pos:end_pos]\n",
        "\n",
        "remove_gutenberg_udf = udf(remove_gutenberg_boilerplate, StringType())\n",
        "\n",
        "print(\"QUESTION 11 - TF-IDF AND BOOK SIMILARITY\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "# Cleaning text\n",
        "preprocessed_df = books_df.select(\n",
        "    col(\"file_name\"),\n",
        "    remove_gutenberg_udf(col(\"text\")).alias(\"clean_text\")\n",
        ")\n",
        "\n",
        "# Lowercaseing and removing punctuation\n",
        "preprocessed_df = preprocessed_df.withColumn(\n",
        "    \"clean_text\",\n",
        "    regexp_replace(lower(col(\"clean_text\")), r'[^\\w\\s]', ' ')\n",
        ").withColumn(\n",
        "    \"clean_text\",\n",
        "    regexp_replace(col(\"clean_text\"), r'\\s+', ' ')\n",
        ").withColumn(\n",
        "    \"clean_text\",\n",
        "    trim(col(\"clean_text\"))\n",
        ")\n",
        "\n",
        "# Tokenizing\n",
        "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
        "tokenized_df = tokenizer.transform(preprocessed_df)\n",
        "\n",
        "# Removing stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "filtered_df = remover.transform(tokenized_df)\n",
        "\n",
        "print(\"Text preprocessing complete\")\n",
        "filtered_df.select(\"file_name\", size(\"filtered_words\").alias(\"word_count\")).show(10)\n",
        "\n",
        "# Calculating TF (Term Frequency)\n",
        "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\",\n",
        "                     vocabSize=5000, minDF=2.0)\n",
        "cv_model = cv.fit(filtered_df)\n",
        "tf_df = cv_model.transform(filtered_df)\n",
        "\n",
        "vocab = cv_model.vocabulary\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "# Calculating IDF (Inverse Document Frequency)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
        "idf_model = idf.fit(tf_df)\n",
        "tfidf_df = idf_model.transform(tf_df)\n",
        "\n",
        "print(\"TF-IDF calculation complete\")\n",
        "tfidf_df.select(\"file_name\", \"tfidf_features\").show(5, truncate=100)\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    # Calculating cosine similarity between two vectors\n",
        "\n",
        "    from pyspark.ml.linalg import SparseVector\n",
        "\n",
        "    if isinstance(v1, SparseVector):\n",
        "        v1 = v1.toArray()\n",
        "    else:\n",
        "        v1 = np.array(v1)\n",
        "\n",
        "    if isinstance(v2, SparseVector):\n",
        "        v2 = v2.toArray()\n",
        "    else:\n",
        "        v2 = np.array(v2)\n",
        "\n",
        "    dot_product = np.dot(v1, v2)\n",
        "    norm1 = np.linalg.norm(v1)\n",
        "    norm2 = np.linalg.norm(v2)\n",
        "\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return float(dot_product / (norm1 * norm2))\n",
        "\n",
        "# Target book for similarity comparison\n",
        "TARGET_BOOK = \"10.txt\"\n",
        "\n",
        "# Getting target book vector\n",
        "target_row = tfidf_df.filter(col(\"file_name\") == TARGET_BOOK).first()\n",
        "\n",
        "if target_row:\n",
        "    target_vec = target_row[\"tfidf_features\"]\n",
        "    target_broadcast = spark.sparkContext.broadcast(target_vec)\n",
        "\n",
        "    # Calculating similarities\n",
        "    similarity_udf = udf(lambda v: float(cosine_similarity(target_broadcast.value, v)), FloatType())\n",
        "\n",
        "    similarity_df = tfidf_df.withColumn(\n",
        "        \"similarity\",\n",
        "        similarity_udf(col(\"tfidf_features\"))\n",
        "    )\n",
        "\n",
        "    # Top similar books\n",
        "    print(f\"\\nTop 6 Most Similar Books to '{TARGET_BOOK}'\")\n",
        "    similarity_df.select(\"file_name\", \"similarity\") \\\n",
        "        .orderBy(desc(\"similarity\")) \\\n",
        "        .show(6)\n",
        "\n",
        "    # Excluding target book itself\n",
        "    print(f\"\\nTop 5 Similar Books (excluding '{TARGET_BOOK}'):\")\n",
        "    top_similar = similarity_df.filter(col(\"file_name\") != TARGET_BOOK) \\\n",
        "        .select(\"file_name\", \"similarity\") \\\n",
        "        .orderBy(desc(\"similarity\")) \\\n",
        "        .limit(5)\n",
        "\n",
        "    top_similar.show()\n",
        "else:\n",
        "    print(f\"Book '{TARGET_BOOK}' not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iXtP_5v4vIY",
        "outputId": "7c6de824-1fbe-45c8-833d-465363965505"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION 11 - TF-IDF AND BOOK SIMILARITY\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Text preprocessing complete\n",
            "+---------+----------+\n",
            "|file_name|word_count|\n",
            "+---------+----------+\n",
            "|  423.txt|     32302|\n",
            "|  152.txt|     29013|\n",
            "|   26.txt|     45963|\n",
            "|   81.txt|     42554|\n",
            "|  456.txt|     20768|\n",
            "|  148.txt|     31530|\n",
            "|  323.txt|     30450|\n",
            "|  465.txt|     26909|\n",
            "|   30.txt|    468762|\n",
            "|  286.txt|     72833|\n",
            "+---------+----------+\n",
            "only showing top 10 rows\n",
            "Vocabulary size: 5000\n",
            "TF-IDF calculation complete\n",
            "+---------+----------------------------------------------------------------------------------------------------+\n",
            "|file_name|                                                                                      tfidf_features|\n",
            "+---------+----------------------------------------------------------------------------------------------------+\n",
            "|  423.txt|(5000,[0,1,2,4,5,6,7,8,9,11,13,14,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30,31,33,35,36,37,38,...|\n",
            "|  152.txt|(5000,[0,1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,18,19,21,22,23,24,25,26,27,28,29,30,31,33,34,35,36,3...|\n",
            "|   26.txt|(5000,[0,1,2,3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,3...|\n",
            "|   81.txt|(5000,[0,1,2,3,4,5,6,7,8,9,11,13,14,15,16,18,19,20,21,22,23,24,25,26,27,28,29,30,31,33,34,35,36,3...|\n",
            "|  456.txt|(5000,[0,1,2,3,4,5,6,7,8,9,10,11,13,14,15,16,18,19,20,21,22,23,24,25,26,27,28,29,30,31,33,35,36,3...|\n",
            "+---------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Top 6 Most Similar Books to '10.txt'\n",
            "+---------+----------+\n",
            "|file_name|similarity|\n",
            "+---------+----------+\n",
            "|   10.txt|       1.0|\n",
            "|   26.txt| 0.4270563|\n",
            "|   30.txt|0.40656522|\n",
            "|   17.txt|0.33204708|\n",
            "|  397.txt|0.32933706|\n",
            "|  359.txt|0.22674192|\n",
            "+---------+----------+\n",
            "only showing top 6 rows\n",
            "\n",
            "Top 5 Similar Books (excluding '10.txt'):\n",
            "+---------+----------+\n",
            "|file_name|similarity|\n",
            "+---------+----------+\n",
            "|   26.txt| 0.4270563|\n",
            "|   30.txt|0.40656522|\n",
            "|   17.txt|0.33204708|\n",
            "|  397.txt|0.32933706|\n",
            "|  359.txt|0.22674192|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 12: AUTHOR INFLUENCE NETWORK\n",
        "\n",
        "from pyspark.sql.functions import countDistinct, coalesce, lit, desc, avg, count\n",
        "\n",
        "def extract_author(text):\n",
        "    if not text: return None\n",
        "    match = re.search(r'Author:\\s*(.+?)(?:\\r?\\n)', text, re.IGNORECASE)\n",
        "    if match: return match.group(1).strip()\n",
        "    match = re.search(r'\\bby\\s+([A-Z][a-zA-Z\\s\\.,]+?)(?:\\r?\\n|\\s{2,})', text)\n",
        "    if match: return match.group(1).strip()\n",
        "    return None\n",
        "\n",
        "extract_author_udf = udf(extract_author, StringType())\n",
        "\n",
        "print(\"QUESTION 12 - AUTHOR INFLUENCE NETWORK\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "author_df = books_df.select(\n",
        "    col(\"file_name\"),\n",
        "    extract_author_udf(col(\"text\")).alias(\"author\"),\n",
        "    extract_release_date_udf(col(\"text\")).alias(\"release_date\")\n",
        ").withColumn(\n",
        "    \"release_year\",\n",
        "    parse_year_udf(col(\"release_date\"))\n",
        ").filter(\n",
        "    (col(\"author\").isNotNull()) & (col(\"release_year\").isNotNull())\n",
        ")\n",
        "\n",
        "print(\"\\nAuthors and Release Years\")\n",
        "author_df.show(15, truncate=50)\n",
        "print(f\"Total books with author and year: {author_df.count()}\")\n",
        "\n",
        "INFLUENCE_WINDOW = 5  # years\n",
        "\n",
        "print(f\"\\nBuilding Influence Network (Window: {INFLUENCE_WINDOW} years)\")\n",
        "\n",
        "# Self-join to find influence relationships\n",
        "influence_edges = author_df.alias(\"a1\").join(\n",
        "    author_df.alias(\"a2\"),\n",
        "    (col(\"a1.author\") != col(\"a2.author\")) &\n",
        "    (col(\"a1.release_year\") <= col(\"a2.release_year\")) &\n",
        "    (col(\"a2.release_year\") - col(\"a1.release_year\") <= INFLUENCE_WINDOW)\n",
        ").select(\n",
        "    col(\"a1.author\").alias(\"author1\"),\n",
        "    col(\"a2.author\").alias(\"author2\"),\n",
        "    col(\"a1.release_year\").alias(\"year1\"),\n",
        "    col(\"a2.release_year\").alias(\"year2\")\n",
        ").distinct()\n",
        "\n",
        "print(f\"Total influence edges: {influence_edges.count()}\")\n",
        "print(\"\\nSample Influence Relationships\")\n",
        "influence_edges.show(15, truncate=50)\n",
        "\n",
        "# Out-degree: authors influenced by author1\n",
        "out_degree = influence_edges.groupBy(\"author1\") \\\n",
        "    .agg(countDistinct(\"author2\").alias(\"out_degree\")) \\\n",
        "    .withColumnRenamed(\"author1\", \"author\")\n",
        "\n",
        "# In-degree: authors who influenced author2\n",
        "in_degree = influence_edges.groupBy(\"author2\") \\\n",
        "    .agg(countDistinct(\"author1\").alias(\"in_degree\")) \\\n",
        "    .withColumnRenamed(\"author2\", \"author\")\n",
        "\n",
        "# Combining degrees\n",
        "all_authors = author_df.select(\"author\").distinct()\n",
        "\n",
        "degree_df = all_authors.alias(\"a\") \\\n",
        "    .join(in_degree.alias(\"i\"), col(\"a.author\") == col(\"i.author\"), \"left\") \\\n",
        "    .join(out_degree.alias(\"o\"), col(\"a.author\") == col(\"o.author\"), \"left\") \\\n",
        "    .select(\n",
        "        col(\"a.author\"),\n",
        "        coalesce(col(\"i.in_degree\"), lit(0)).alias(\"in_degree\"),\n",
        "        coalesce(col(\"o.out_degree\"), lit(0)).alias(\"out_degree\")\n",
        "    )\n",
        "\n",
        "print(\"\\nTop 5 Authors by In-Degree (Most Influenced)\")\n",
        "degree_df.orderBy(desc(\"in_degree\")).show(5, truncate=False)\n",
        "\n",
        "print(\"\\nTop 5 Authors by Out-Degree (Most Influential)\")\n",
        "degree_df.orderBy(desc(\"out_degree\")).show(5, truncate=False)\n",
        "\n",
        "# Network statistics\n",
        "print(\"\\nNetwork Statistics\")\n",
        "total_authors = degree_df.count()\n",
        "total_edges = influence_edges.count()\n",
        "avg_in = degree_df.agg(avg(\"in_degree\")).first()[0]\n",
        "avg_out = degree_df.agg(avg(\"out_degree\")).first()[0]\n",
        "\n",
        "print(f\"Total authors: {total_authors}\")\n",
        "print(f\"Total influence relationships: {total_edges}\")\n",
        "print(f\"Average in-degree: {avg_in:.2f}\")\n",
        "print(f\"Average out-degree: {avg_out:.2f}\")\n",
        "\n",
        "degree_df.describe(\"in_degree\", \"out_degree\").show()\n",
        "\n",
        "# Create output directory\n",
        "!mkdir -p /content/output\n",
        "\n",
        "# Save results\n",
        "print(\"Saving Results..\")\n",
        "\n",
        "metadata_df.coalesce(1).write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\").csv(\"/content/output/metadata\")\n",
        "\n",
        "if target_row:\n",
        "    top_similar.coalesce(1).write.mode(\"overwrite\") \\\n",
        "        .option(\"header\", \"true\").csv(\"/content/output/similarity\")\n",
        "\n",
        "degree_df.coalesce(1).write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\").csv(\"/content/output/network\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J-v7td15eLp",
        "outputId": "a233c558-44fe-4411-ca3d-75be32e56513"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION 12 - AUTHOR INFLUENCE NETWORK\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Authors and Release Years\n",
            "+---------+------------------------------+----------------+------------+\n",
            "|file_name|                        author|    release_date|release_year|\n",
            "+---------+------------------------------+----------------+------------+\n",
            "|  423.txt|            Arthur Conan Doyle|February 3, 2008|        2008|\n",
            "|  152.txt|               Ruth M. Sprague|    August, 1994|        1994|\n",
            "|   26.txt|                   John Milton|  February, 1992|        1992|\n",
            "|   81.txt|          Edgar Rice Burroughs|   June 23, 2008|        2008|\n",
            "|  456.txt|                   H. G. Wells|   July 22, 2005|        2005|\n",
            "|  148.txt|             Benjamin Franklin|    May 22, 2008|        2008|\n",
            "|  323.txt|               Rudyard Kipling|   June 29, 2008|        2008|\n",
            "|  465.txt|          Stewart Edward White|     March, 1996|        1996|\n",
            "|   30.txt|                       Various|     April, 1992|        1992|\n",
            "|  286.txt|          Gene Stratton Porter|   April 3, 2008|        2008|\n",
            "|  118.txt|Electronic Frontier Foundation|     March, 1994|        1994|\n",
            "|  317.txt|           Joseph Rodman Drake|January 18, 2007|        2007|\n",
            "|   39.txt|                       Ed Krol| September, 1992|        1992|\n",
            "|  102.txt|   Mark Twain (Samuel Clemens)|   January, 1994|        1994|\n",
            "|  349.txt|          Gene Stratton Porter|   October, 1995|        1995|\n",
            "+---------+------------------------------+----------------+------------+\n",
            "only showing top 15 rows\n",
            "Total books with author and year: 97\n",
            "\n",
            "Building Influence Network (Window: 5 years)\n",
            "Total influence edges: 2605\n",
            "\n",
            "Sample Influence Relationships\n",
            "+----------------------------------+---------------------------+-----+-----+\n",
            "|                           author1|                    author2|year1|year2|\n",
            "+----------------------------------+---------------------------+-----+-----+\n",
            "|                           Various|               Omar Khayyam| 1992| 1995|\n",
            "|                   Ruth M. Sprague|                Edna Ferber| 1994| 1996|\n",
            "|                       H. G. Wells|       Edgar Rice Burroughs| 2005| 2008|\n",
            "|                           Various|Mark Twain (Samuel Clemens)| 1992| 1994|\n",
            "|              Gene Stratton Porter|          Project Gutenberg| 2008| 2011|\n",
            "|            Frances Jenkins Olcott|             Vachel Lindsay| 1995| 1995|\n",
            "|United States Bureau of the Census|       Gene Stratton Porter| 1992| 1995|\n",
            "|                   Rudyard Kipling|               Andrew Lang.| 2008| 2008|\n",
            "|                      Thomas Hardy|          Project Gutenberg| 2008| 2011|\n",
            "|                   Kenneth Grahame|                Don Marquis| 1995| 1996|\n",
            "|                       H. G. Wells|               Daniel Defoe| 2005| 2008|\n",
            "|              Gene Stratton Porter|               Daniel Defoe| 2008| 2008|\n",
            "|    Electronic Frontier Foundation|               Omar Khayyam| 1994| 1995|\n",
            "|                     Horatio Alger|         Christopher Morley| 2006| 2008|\n",
            "|           Frances Hodgson Burnett|          Project Gutenberg| 2006| 2011|\n",
            "+----------------------------------+---------------------------+-----+-----+\n",
            "only showing top 15 rows\n",
            "\n",
            "Top 5 Authors by In-Degree (Most Influenced)\n",
            "+------------------------------+---------+----------+\n",
            "|author                        |in_degree|out_degree|\n",
            "+------------------------------+---------+----------+\n",
            "|Arthur Conan Doyle            |66       |46        |\n",
            "|Edna Ferber                   |66       |45        |\n",
            "|Gene Stratton Porter          |59       |53        |\n",
            "|Andrew Barton 'Banjo' Paterson|59       |53        |\n",
            "|Edith Wharton                 |59       |53        |\n",
            "+------------------------------+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Top 5 Authors by Out-Degree (Most Influential)\n",
            "+------------------------------+---------+----------+\n",
            "|author                        |in_degree|out_degree|\n",
            "+------------------------------+---------+----------+\n",
            "|Thomas Hardy                  |48       |70        |\n",
            "|Various                       |48       |61        |\n",
            "|Jane Austen                   |53       |56        |\n",
            "|Gene Stratton Porter          |59       |53        |\n",
            "|Andrew Barton 'Banjo' Paterson|59       |53        |\n",
            "+------------------------------+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Network Statistics\n",
            "Total authors: 73\n",
            "Total influence relationships: 2605\n",
            "Average in-degree: 31.84\n",
            "Average out-degree: 31.84\n",
            "+-------+------------------+------------------+\n",
            "|summary|         in_degree|        out_degree|\n",
            "+-------+------------------+------------------+\n",
            "|  count|                73|                73|\n",
            "|   mean|31.835616438356166|31.835616438356166|\n",
            "| stddev|17.086392976926064|15.881517645984779|\n",
            "|    min|                 0|                 0|\n",
            "|    max|                66|                70|\n",
            "+-------+------------------+------------------+\n",
            "\n",
            "Saving Results..\n"
          ]
        }
      ]
    }
  ]
}